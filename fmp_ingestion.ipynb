{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9533f6b8-b9f0-431f-8083-13dc1bdea45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\"Restoring serverless notebook session state...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefa8aed-4840-4041-99e1-839fd610013f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL VOLUME IF NOT EXISTS stock_pipeline.default.bronze_volume\n",
    "LOCATION 's3://stock-pipeline-data-dev-mc/bronze/';\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1427ab1c-07f2-41de-b8fd-9ca7275c360f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --force-reinstall --no-cache-dir /Volumes/stock_pipeline/default/wheel_volume/stock_pipeline-0.1.dev65-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2f2e4b9-018a-43d4-899c-45ee69633818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0626f7-e86c-4350-84d8-ce006e30b2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.createDataFrame([(k,) for k in dbutils.secrets.list(\"fmp_scope\")], [\"key\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8eb02c2-7022-47d8-90ef-746b430b5480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from datetime import datetime, timedelta\n",
    "from bronze.utils import AsyncFMPClient, FMPConfig, create_spark_session\n",
    "from bronze.ingestion.fmp import ingest_ticker_bronze\n",
    "from bronze.ingestion.schemas import get_bronze_schemas\n",
    "\n",
    "# Set up your environment\n",
    "import os\n",
    "os.environ['FMP_API_KEY'] = dbutils.secrets.get(\"fmp_scope\", \"FMP_API_KEY\")\n",
    "os.environ['S3_BUCKET_BRONZE'] = dbutils.secrets.get(\"fmp_scope\", \"S3_BUCKET_BRONZE\")\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = dbutils.secrets.get(\"fmp_scope\", \"AWS_ACCESS_KEY_ID\")\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = dbutils.secrets.get(\"fmp_scope\", \"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "async def test_ingestion():\n",
    "    # Initialize components (same as main() does)\n",
    "    config = FMPConfig(api_key=os.environ[\"FMP_API_KEY\"])\n",
    "    client = AsyncFMPClient(config)\n",
    "    spark = create_spark_session()\n",
    "    schemas = get_bronze_schemas()\n",
    "\n",
    "    # Define parameters (equivalent to --tickers AAPL --backfill)\n",
    "    tickers = [\"AAPL\", \"MSFT\"]\n",
    "    backfill = False  # Set to True for 5-year history\n",
    "\n",
    "    # Calculate date range\n",
    "    if backfill:\n",
    "        to_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        from_date = (datetime.now() - timedelta(days=5*365)).strftime(\"%Y-%m-%d\")      \n",
    "        print(f\"ðŸ“… Backfill mode: {from_date} to {to_date}\")\n",
    "    else:\n",
    "        yesterday = datetime.now() - timedelta(days=1)\n",
    "        from_date = to_date = yesterday.strftime(\"%Y-%m-%d\")\n",
    "        print(f\"ðŸ“… Daily mode: {from_date}\")\n",
    "\n",
    "    # Process tickers\n",
    "    results = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"ðŸ”„ Processing {ticker}...\")\n",
    "        result = await ingest_ticker_bronze(client, spark, ticker, from_date if from_date else '', to_date if to_date else '', schemas)\n",
    "        results.append(result)\n",
    "        print(f\"âœ… {ticker} complete: {result['record_counts']}\")\n",
    "\n",
    "    spark.stop()\n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "results = await test_ingestion()\n",
    "print(f\"ðŸŽ¯ Ingestion complete: {len(results)} tickers processed\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5118489541844870,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "fmp_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
