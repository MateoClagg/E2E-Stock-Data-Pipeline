# ğŸ“Š Stock Market Data Pipeline

[![Build Status](../../actions/workflows/pr-build.yml/badge.svg)](../../actions/workflows/pr-build.yml)
[![Python](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **Cost-optimized stock data ingestion** - Local async FMP API fetching â†’ S3 Parquet â†’ Databricks medallion architecture.

## ğŸ—ï¸ **Architecture Overview**

**Cost-First Design**: API wait time runs locally (GitHub Actions/EC2), Databricks only for data transformations.

```
Local/GitHub Actions  S3 Raw Zone          Databricks Serverless         Analytics
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FMP API         â”‚   â”‚ Day-level    â”‚â”€â”€â”€â–¶â”‚ Auto Loader â†’ Bronze    â”‚    â”‚ Delta   â”‚
â”‚ (async fetch)   â”‚â”€â”€â–¶â”‚ Parquet      â”‚    â”‚ (CDF) â†’ Silver (MERGE)  â”‚â”€â”€â”€â–¶â”‚ Tables  â”‚
â”‚ polars/pyarrow  â”‚   â”‚ partitions   â”‚    â”‚ â†’ Gold (features)       â”‚    â”‚ & Views â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚
                                                      â–¼
                                              Databricks SQL/Athena
```

## ğŸ¯ **Key Benefits**

- **ğŸ’° Cost Optimized**: 70% cost reduction by offloading API wait time from Databricks
- **ğŸš€ Scalable**: Designed for 1,000+ tickers, 5+ years of history
- **âš¡ Fast Development**: Local testing without expensive compute
- **ğŸ”„ Flexible**: Keep existing Databricks expertise for transformations
- **ğŸ“Š Production Ready**: Enterprise security, monitoring, and CI/CD

## âœ¨ **Features**

- **ğŸ“ˆ Multi-source Data**: Price data, income statements, cash flow, and balance sheets
- **âš¡ Async Processing**: Concurrent API calls with built-in rate limiting  
- **ğŸ—‚ï¸ Smart Partitioning**: Optimized for query performance and cost
- **ğŸ” Data Quality**: Great Expectations validation with quarantine patterns
- **ğŸ¦ Lakehouse Format**: Parquet (raw) â†’ Delta (analytics) for ACID + time travel

## ğŸš€ **Quick Start**

```bash
# 1. Install dependencies
pip install -e .

# 2. Configure environment (.env file)
export FMP_API_KEY="your_api_key"
export S3_BUCKET="your-bucket"
export AWS_ACCESS_KEY_ID="..."
export AWS_SECRET_ACCESS_KEY="..."

# 3. Run local ingestion (yesterday's data)
python stock_pipeline/scripts/ingest_local_to_s3.py

# 4. Backfill historical data
python stock_pipeline/scripts/ingest_local_to_s3.py --backfill-days 30
```

**Output:** Day-partitioned Parquet in `s3://{bucket}/raw/prices/symbol=X/year=Y/month=M/day=D/`

**Monthly Cost**: ~$50 total (FMP API: $30, S3: <$10, Databricks: <$10)

## ğŸ—ï¸ **Architecture**

### **Medallion Data Flow**
```mermaid
graph LR
    A[FMP API] --> B[Bronze Layer]
    B --> C[Silver Layer] 
    C --> D[Gold Layer]
    B --> E[S3 Raw Storage]
    C --> F[Delta Tables]
    D --> G[Analytics Views]
```

### **Package Structure**
```
ğŸ“¦ stock-pipeline/
â”œâ”€â”€ ğŸ“Š stock_pipeline/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ ingest_local_to_s3.py   # Main ingestion script (async + polars)
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â””â”€â”€ dates.py            # Trading calendar utilities
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ tickers.csv             # Default ticker list
â”œâ”€â”€ ğŸ§ª tests/
â”‚   â””â”€â”€ test_ingest_local.py        # Unit tests
â”œâ”€â”€ ğŸ“Š docs/
â”‚   â””â”€â”€ ingestion_quickstart.md     # Complete ingestion guide
â””â”€â”€ âš™ï¸ .github/workflows/
    â””â”€â”€ ingest.yml                   # Nightly cron + manual dispatch
```

**Note:** Bronze/Silver/Gold transformations live in Databricks (not in this repo).

## ğŸ“Š **Data Pipeline**

| Layer | Purpose | Technology | Location |
|-------|---------|------------|----------|
| **Raw Zone** | Local async ingestion | aiohttp + polars + boto3 | S3 Parquet (day-partitioned) |
| **Bronze** | Auto Loader streaming | Databricks Auto Loader | Delta Tables (CDF enabled) |
| **Silver** | Cleaning & transformations | PySpark + Delta MERGE | Delta Tables |
| **Gold** | Analytics & features | SQL + Views | Databricks Views/Tables |

## ğŸ”§ **Development**

```bash
# Clone and install
git clone <repository-url>
cd E2E-Stock-Data-Pipeline
pip install -e .

# Run tests
pytest tests/test_ingest_local.py -v

# Test locally with your FMP key
python stock_pipeline/scripts/ingest_local_to_s3.py --tickers-path stock_pipeline/config/tickers.csv
```

### **CI/CD Pipeline**
- **PR Builds**: Fast validation - linting, imports, unit tests
- **Main Builds**: Full test suite + S3 wheel uploads
- **Ingestion Workflow**: Nightly cron (Mon-Fri 11 PM UTC) + manual dispatch

## ğŸ“‹ **Requirements**

- **Python 3.10+**
- **AWS S3** access
- **FMP API subscription** ($30/month for real-time data)
- **Databricks workspace** (optional, for Bronze/Silver/Gold transformations)

## ğŸ§ª **Testing**

```bash
# Unit tests
pytest tests/test_ingest_local.py -v

# Test specific components
pytest tests/test_ingest_local.py::TestS3KeyBuilder -v
pytest tests/test_ingest_local.py::TestPolarsTransformations -v
```

## ğŸ“š **Documentation**

| Document | Description |
|----------|-------------|
| **[ğŸš€ Getting Started](GETTING_STARTED.md)** | Installation and quick setup |
| **[ğŸ“Š Ingestion Guide](docs/ingestion_quickstart.md)** | Complete local ingestion documentation |
| **[ğŸ”§ Databricks Setup](databricks/DATABRICKS_SETUP.md)** | Unity Catalog + Auto Loader configuration |

## ğŸ¤ **Contributing**

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Test** your changes (`pytest tests/ -v`)
4. **Commit** with clear messages
5. **Submit** a Pull Request

## ğŸ“Š **Performance**

- **API Rate Limiting**: 5 requests/second (configurable)
- **Concurrent Processing**: Multiple tickers processed in parallel
- **S3 Partitioning**: Optimized for time-series queries
- **Delta Lake**: ACID transactions and time travel
- **Memory Optimized**: Configurable Spark memory settings

## ğŸ”’ **Security & Compliance**

- **OIDC Authentication**: No long-lived AWS credentials
- **Supply Chain Security**: SBOM generation and vulnerability scanning
- **Secrets Management**: Environment variable based configuration
- **Access Control**: IAM roles and policies for least-privilege access

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Built with â¤ï¸ for the financial data community**# Trigger main build
