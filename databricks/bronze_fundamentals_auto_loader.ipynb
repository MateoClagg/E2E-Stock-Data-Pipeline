{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2acd3d5-c00b-4918-8b43-f2b4f0e590c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set paths to external location (s3 bucket)\n",
    "S3_BUCKET = \"s3://stock-pipeline-data-dev-mc\"\n",
    "CATALOG = \"stock_pipeline\"\n",
    "SCHEMA = \"bronze\"\n",
    "TABLE = \"fmp_fundamentals\"\n",
    "FULL_TABLE_NAME = f\"{CATALOG}.{SCHEMA}.{TABLE}\"\n",
    "\n",
    "# External paths only used if you choose path-based tables instead of UC (we use UC here)\n",
    "BRONZE_TABLE_PATH = f\"{S3_BUCKET}/bronze/fmp_fundamentals/\"\n",
    "CHECKPOINT_PATH = f\"{S3_BUCKET}/_checkpoints/bronze_fundamentals/\"\n",
    "SCHEMA_LOC_PATH = f\"{S3_BUCKET}/_checkpoints/bronze_fundamentals_schema/\"\n",
    "\n",
    "## Ensure Catalog/Schema exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26cba436-dd73-4567-a7b0-76a05fedc5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS stock_pipeline.bronze.fmp_fundamentals\n",
    "USING DELTA\n",
    "LOCATION 's3://stock-pipeline-data-dev-mc/bronze/fmp_fundamentals/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71918ac2-bf5b-4bc5-95dd-3ff561c608c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Auto Loader Stream\n",
    "from pyspark.sql.functions import col, input_file_name, current_timestamp, to_date, to_timestamp\n",
    "\n",
    "# Use wildcard to match all statement types\n",
    "RAW_FUNDAMENTALS_PATH = f\"{S3_BUCKET}/raw/fmp/*/\"\n",
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\") # NDJSON lines (gz supported)\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.schemaLocation\", SCHEMA_LOC_PATH)\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "        .option(\"cloudFiles.includeExistingFiles\", \"false\") # Change to false for only new files\n",
    "        .load(RAW_FUNDAMENTALS_PATH)\n",
    ")\n",
    "\n",
    "\n",
    "bronze_df = (\n",
    "    df\n",
    "    .withColumn(\"as_of_date\", to_date(\"as_of_date\"))\n",
    "    .withColumn(\"fiscal_period_end\", to_date(\"fiscal_period_end\"))\n",
    "    .withColumn(\"filing_date\", to_date(\"filing_date\"))\n",
    "    .withColumn(\"fetched_at_ts\", to_timestamp(\"fetched_at\"))\n",
    "    .withColumn(\"_input_file\", col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"_processing_time\", current_timestamp())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89ded4fe-0973-4f28-b57e-dbc9611e0f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Write to Bronze Delta (Backfill once)\n",
    "q = (\n",
    "    bronze_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/stream\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .option(\"optimizeWrite\", \"true\")\n",
    "    .option(\"autoCompact\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(FULL_TABLE_NAME)\n",
    ")\n",
    "\n",
    "q.awaitTermination()\n",
    "\n",
    "# Enable table conveniences\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {FULL_TABLE_NAME}\n",
    "SET TBLPROPERTIES (\n",
    "    delta.autoOptimize.optimizeWrite = true,\n",
    "    delta.autoOptimize.autoCompact = true,\n",
    "    delta.enableChangeDataFeed = true\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "## Verify\n",
    "bronze = spark.read.table(FULL_TABLE_NAME)\n",
    "\n",
    "print(\"Counts by endpoint:\")\n",
    "display(bronze.groupBy(\"endpoint\").count().orderBy(\"endpoint\"))\n",
    "\n",
    "print(\"Counts by symbol (top 20):\")\n",
    "display(bronze.groupBy(\"symbol\").count().orderBy(col(\"count\").desc()).limit(20))\n",
    "\n",
    "print(\"Sample records:\")\n",
    "display(bronze.select(\"symbol\",\"endpoint\",\"fiscal_period_end\",\"filing_date\",\"fetched_at_ts\").limit(10))\n",
    "\n",
    "\n",
    "## Quick Data Quality\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "\n",
    "nulls = bronze.select(\n",
    "*[col(c).isNull().cast(\"int\").alias(c) for c in [\"symbol\",\"endpoint\",\"payload\",\"hash\"]]\n",
    ").agg(*[_sum(c).alias(c) for c in [\"symbol\",\"endpoint\",\"payload\",\"hash\"]])\n",
    "\n",
    "display(nulls)\n",
    "\n",
    "dupes = bronze.groupBy(\"hash\").count().filter(col(\"count\") > 1)\n",
    "print(\"Duplicate payload hashes:\", dupes.count())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4545024932546578,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_fundamentals_auto_loader",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
