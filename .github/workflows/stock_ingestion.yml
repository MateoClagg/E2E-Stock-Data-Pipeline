name: Daily Stock Ingestion

on:
  schedule:
    - cron: '0 14 * * *' # Runs every day at 8 AM CST (14 UTC)
  workflow_dispatch:      # Allows manual triggering

jobs:
  ingest:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install snowflake-connector-python
        pip install awscli  

    - name: Create .env file from secret
      run: echo "POLYGON_API_KEY=${{ secrets.POLYGON_API_KEY }}" > .env

    - name: Set AWS credentials
      run: |
        mkdir -p ~/.aws
        echo "[default]" > ~/.aws/credentials
        echo "aws_access_key_id=${{ secrets.AWS_ACCESS_KEY_ID }}" >> ~/.aws/credentials
        echo "aws_secret_access_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> ~/.aws/credentials

    - name: Run ingestion script
      run: python stock_pipeline/scripts/polygon_ingest.py

    - name: Upload daily folder to S3
      run: |
        CST_DATE=$(date -u -d '1 day ago 6 hours ago' +'%Y-%m-%d')
        aws s3 cp stock_pipeline/daily/$CST_DATE/ s3://e2e-stock-pipeline-data/stock_pipeline/daily/$CST_DATE/ --recursive
    
    - name: Clean up old local folders after S3 archival
      run: |
        python stock_pipeline/scripts/archive_and_cleanup.py
    
    - name: Trigger Snowflake ingestion
      if: github.ref == 'refs/heads/Add-snowflake-trigger-after-S3-upload'
      env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}
          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}
          SNOWFLAKE_DATABASE: ${{ secrets.SNOWFLAKE_DATABASE }}
          SNOWFLAKE_SCHEMA: ${{ secrets.SNOWFLAKE_SCHEMA }}
          SNOWFLAKE_WAREHOUSE: ${{ secrets.SNOWFLAKE_WAREHOUSE }}
      run: |
          python stock_pipeline/scripts/trigger_snowflake.py
