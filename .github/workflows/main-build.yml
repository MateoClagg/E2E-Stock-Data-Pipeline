name: Main Branch Build

on:
  push:
    branches: [ main ]

permissions:
  contents: read
  actions: read
  id-token: write  # Required for OIDC authentication with AWS

concurrency:
  group: main-build-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel main builds

jobs:
  comprehensive-build:
    name: Comprehensive Build & Test
    runs-on: ubuntu-latest
    timeout-minutes: 10  # Up to 10 minutes for comprehensive testing
    
    strategy:
      matrix:
        python-version: ["3.10", "3.11"]  # Full matrix on main
    
    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4.1.7
      with:
        fetch-depth: 0  # Full history for setuptools_scm

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@39cd14951b08e74b54015e9e001cdefcf80e669f  # v5.1.1
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install build dependencies
      run: |
        python -m pip install --upgrade pip setuptools>=77 wheel build twine
        pip install -r requirements.txt

    - name: Full lint check
      run: |
        pip install ruff
        ruff check . --fix --statistics
      continue-on-error: true

    - name: Clean previous builds
      run: |
        rm -rf dist/ build/ *.egg-info/
        
    - name: Build package (wheel + sdist)
      run: |
        python -m build  # Both wheel and sdist
        
    - name: Verify build artifacts
      run: |
        ls -la dist/
        twine check dist/*

    #- name: Generate SBOM and security audit
     # run: |
      #  # Generate SBOM (Software Bill of Materials)
      #  python -m cyclonedx_bom -o sbom.json
        
        # Run security audit
     #   pip-audit --desc --output audit-report.json --format json dist/*.whl || {
     #     echo "‚ö†Ô∏è  Security vulnerabilities found (see audit-report.json)"
     #   }

    - name: Comprehensive package test
      run: |
        # Install the built wheel
        pip install dist/*.whl

        # Simple smoke test without complex indentation
        python -c "import stock_pipeline; from stock_pipeline.scripts.ingest_fmp_prices import Config, FMPClient; print('‚úì All modules imported'); print(f'‚úì Version: {stock_pipeline.__version__}')"

    - name: Run comprehensive tests
      run: |
        pip install pytest pytest-asyncio pytest-cov
        # Full test suite with coverage
        pytest tests/ -v --cov=stock_pipeline --cov-report=xml -m "not integration" --tb=short

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: false

    - name: Generate version info
      id: version
      run: |
        pip install setuptools-scm[toml]
        VERSION=$(python -c "from setuptools_scm import get_version; print(get_version())")
        echo "version=$VERSION" >> $GITHUB_OUTPUT
        echo "Generated version: $VERSION"

    - name: Upload main artifacts
      uses: actions/upload-artifact@330a01c490aca151604b8cf639adc76d48f6c5d4  # v5.0.0
      with:
        name: main-package-${{ matrix.python-version }}-${{ steps.version.outputs.version }}
        path: |
          dist/
          sbom.json
          audit-report.json
        retention-days: 14
        if-no-files-found: error

    - name: Configure AWS credentials for S3 upload
      if: github.ref == 'refs/heads/main' && matrix.python-version == '3.11'
      uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502  # v4.0.2
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
        aws-region: ${{ secrets.AWS_REGION }}

    - name: Upload wheels to S3 for Databricks Unity Catalog
      if: github.ref == 'refs/heads/main' && matrix.python-version == '3.11'
      run: |
        echo "üì¶ Uploading main snapshot wheels to S3..."
        S3_PATH="s3://${{ vars.S3_WHEELS_BUCKET }}/wheels/stock-pipeline/main/"
        
        # Clean up old files first
        echo "üßπ Cleaning up old wheel files..."
        aws s3 rm "${S3_PATH}" --recursive || echo "No existing files to clean"
        
        # Find the wheel file and upload with stable name
        WHEEL_FILE=$(find dist/ -name "*.whl" | head -1)
        if [ -n "$WHEEL_FILE" ]; then
          # Upload wheel with stable name for Databricks
          aws s3 cp "$WHEEL_FILE" \
            "${S3_PATH}stock_pipeline-latest.whl" \
            --acl bucket-owner-full-control
          echo "‚úÖ Wheel uploaded as: ${S3_PATH}stock_pipeline-latest.whl"
        else
          echo "‚ùå No wheel file found in dist/"
          exit 1
        fi

